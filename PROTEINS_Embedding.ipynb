{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lqL-LQstoi0"
   },
   "source": [
    "# Classification on the PROTEINS Dataset\n",
    "\n",
    "We're going to do our walkthrough with [Spektral](https://graphneural.network/getting-started/) for Python to demonstrate the implementation of GCNs. \n",
    "\n",
    "The PROTEINS dataset contains 1,113 graphs, all representative of the structure of different proteins. For our walkthrough, we'll mostly be focused in on building a GCN, but I encourage you to dive deeper into this dataset later if you're interested in learning more about the feature representations, or what we're classifying! \n",
    "\n",
    "Spektral is a library for Python for Graph Neural Networks, built on Tensorflow and Keras. Another great alternative is PyTorch Geometric. \n",
    "\n",
    "While Spektral is fabulous for quickly getting a model up and running (like for this walkthrough) PyTorch Geometric will be more comfortable  for those who prefer PyTorch over Tensorflow & Keras, it just takes a little bit more time to get set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_884Zbltkwo"
   },
   "outputs": [],
   "source": [
    "# Uncomment me and run this cell!\n",
    "# !pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fw3cynAtwQOE",
    "outputId": "dbfb605f-2ca3-4c5e-a7c4-264de65e60e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PROTEINS dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 447k/447k [00:00<00:00, 496kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PROTEINS.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TUDataset(n_graphs=1113)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the PROTEINS dataset\n",
    "from spektral.datasets import TUDataset\n",
    "\n",
    "# Spectral provides the TUDataset class, which contains benchmark datasets for graph classification\n",
    "data = TUDataset('PROTEINS')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3AZCWahwwj7A"
   },
   "outputs": [],
   "source": [
    "# Since we want to utilize the Spektral GCN layer, we want to follow the original paper for this method and perform some preprocessing:\n",
    "from spektral.transforms import GCNFilter\n",
    "\n",
    "# Apply the built-in filter to all of our data:\n",
    "data.apply(GCNFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uDNI3sTt5mJQ"
   },
   "outputs": [],
   "source": [
    "# Split our train and test data. This just splits based on the first 80%/second 20% which isn't entirely ideal, so we'll shuffle the data first.\n",
    "import numpy as np\n",
    "\n",
    "np.random.shuffle(data)\n",
    "split = int(0.8 * len(data))\n",
    "data_train, data_test = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lwvnn79Ewm7M"
   },
   "outputs": [],
   "source": [
    "# Spektral is built on top of Keras, so we can use the Keras functional API to build a model that first embeds,\n",
    "# then sums the nodes together (global pooling), then classifies the result with a dense softmax layer\n",
    "\n",
    "# First, let's import the necessary layers:\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from spektral.layers import GCNConv, GlobalSumPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PrTSQXkZyW78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 2\n"
     ]
    }
   ],
   "source": [
    "# Now, we can use model subclassing to define our model:\n",
    "\n",
    "class ProteinsGNN(Model):\n",
    "  \n",
    "  def __init__(self, n_hidden, n_labels):\n",
    "    super().__init__()\n",
    "    # Define our GCN layer with our n_hidden layers\n",
    "    self.graph_conv = GCNConv(n_hidden)\n",
    "    # Define our global pooling layer\n",
    "    self.pool = GlobalSumPool()\n",
    "    # Define our dropout layer, initialize dropout freq. to .5 (50%)\n",
    "    self.dropout = Dropout(0.5)\n",
    "    # Define our Dense layer, with softmax activation function\n",
    "    self.dense = Dense(n_labels, 'softmax')\n",
    "\n",
    "  # Define class method to call model on input\n",
    "  def call(self, inputs):\n",
    "    out = self.graph_conv(inputs)\n",
    "    out = self.dropout(out)\n",
    "    out = self.pool(out)\n",
    "    out = self.dense(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6CWW1urKzRrU"
   },
   "outputs": [],
   "source": [
    "# Instantiate our model for training\n",
    "model = ProteinsGNN(32, data.n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WG2YY6CMzf3I"
   },
   "outputs": [],
   "source": [
    "# Compile model with our optimizer (adam) and loss function\n",
    "model.compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qw1hlui8zpYg"
   },
   "outputs": [],
   "source": [
    "# Here's the trick - we can't just call Keras' fit() method on this model.\n",
    "# Instead, we have to use Loaders, which Spektral walks us through. Loaders create mini-batches by iterating over the graph\n",
    "# Since we're using Spektral for an experiment, for our first trial we'll use the recommended loader in the getting started tutorial\n",
    "\n",
    "# TODO: read up on modes and try other loaders later\n",
    "from spektral.data import BatchLoader\n",
    "\n",
    "loader = BatchLoader(data_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmJ70FZy0Jgw",
    "outputId": "51895e32-4db6-4224-a5a0-76f9f5bf4246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 1s 13ms/step - loss: 12.2807\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 6.1447\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 5.8806\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 5.4637\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 5.0572\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 4.9233\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 4.3972\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 4.7675\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 5.1816\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 4.6123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84ae419400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train! We don't need to specify a batch size, since our loader is basically a generator\n",
    "# But we do need to specify the steps_per_epoch parameter\n",
    "\n",
    "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9GOzdAb01Ei4"
   },
   "outputs": [],
   "source": [
    "# To evaluate, let's instantiate another loader to test\n",
    "\n",
    "test_loader = BatchLoader(data_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 13ms/step - loss: 3.7211\n",
      "Test loss: 3.7211432456970215\n"
     ]
    }
   ],
   "source": [
    "# And feed it to our model by calling .load()\n",
    "\n",
    "loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)\n",
    "\n",
    "print('Test loss: {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PROTEINS_Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
